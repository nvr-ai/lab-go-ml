// Package detectors - ONNX model inference.
package detectors

import (
	"context"
	"fmt"
	"image"
	"log"
	"os"
	"sort"
	"sync"

	"github.com/nvr-ai/go-ml/inference"
	"github.com/nvr-ai/go-ml/inference/providers"
	ort "github.com/yalue/onnxruntime_go"

	"gocv.io/x/gocv"
)

// ObjectDetectionResult represents a detected object
type ObjectDetectionResult struct {
	Box       image.Rectangle
	Score     float32
	ClassID   int
	ClassName string
}

// ONNXDetector handles ONNX model inference using gocv.ReadNet()
type ONNXDetector struct {
	session             *inference.Session
	modelPath           string
	inputShape          image.Point
	confidenceThreshold float32
	nmsThreshold        float32
	relevantClasses     map[string]bool
	initialized         bool
	mu                  sync.RWMutex
	net                 gocv.Net
	outputNames         []string
}

// NewSession creates a new ONNX detector.
//
// Arguments:
//   - config: The configuration for the ONNX detector.
//
// Returns:
//   - *Session: The ONNX detector session.
//   - error: An error if the session creation fails.
func NewSession(config Config) (*inference.Session, error) {
	// Check if the shared library exists before trying to use it.
	libPath := inference.GetSharedLibPath()
	if _, err := os.Stat(libPath); os.IsNotExist(err) {
		return nil, fmt.Errorf("ONNX Runtime library not found at %s. On macOS ARM64, you need to build ONNX Runtime from source or disable ONNX Runtime. Error: %w", libPath, err)
	}

	ort.SetEnvironmentLogLevel(ort.LoggingLevelVerbose)
	ort.SetSharedLibraryPath(libPath)

	err := ort.InitializeEnvironment()
	if err != nil {
		return nil, fmt.Errorf("error initializing ORT environment: %w", err)
	}

	inputShape := ort.NewShape(1, 3, 640, 640)
	inputTensor, err := ort.NewEmptyTensor[float32](inputShape)
	if err != nil {
		return nil, fmt.Errorf("error creating input tensor: %w", err)
	}

	outputShape := ort.NewShape(1, 84, 8400)
	outputTensor, err := ort.NewEmptyTensor[float32](outputShape)
	if err != nil {
		inputTensor.Destroy() // Clean up input tensor if output tensor creation fails
		return nil, fmt.Errorf("error creating output tensor: %w", err)
	}

	options, err := ort.NewSessionOptions()
	if err != nil {
		return nil, fmt.Errorf("error creating ORT session options: %w", err)
	}
	defer options.Destroy()

	// Sets the number of threads used to parallelize execution within onnxruntime graph nodes. A value of 0 uses the default number of threads.
	options.SetIntraOpNumThreads(4)
	// Sets the number of threads used to parallelize execution across separate onnxruntime graph nodes. A value of 0 uses the default number of threads.
	options.SetInterOpNumThreads(2)
	// Sets the optimization level to apply when loading a graph.
	options.SetGraphOptimizationLevel(ort.GraphOptimizationLevelEnableExtended)

	if config.Provider.OptimizationConfig.ExecutionProviders[0].Provider == providers.CoreMLExecutionProvider {
		err = options.AppendExecutionProviderCoreML(0)
		if err != nil {
			return nil, fmt.Errorf("error enabling CoreML: %w", err)
		}
	}

	if config.Provider.OptimizationConfig.ExecutionProviders[0].Provider == providers.OpenVINOExecutionProvider {
		err = options.AppendExecutionProviderOpenVINO(map[string]string{
			"device_id":      "0",
			"device_type":    "CPU",
			"precision":      "FP32",
			"num_of_threads": "4",
		})
		if err != nil {
			return nil, fmt.Errorf("error enabling OpenVINO: %w", err)
		}
	}

	session, err := ort.NewAdvancedSession(
		config.Provider.ModelPath,
		[]string{"images"},
		[]string{"output0"},
		[]ort.ArbitraryTensor{inputTensor},
		[]ort.ArbitraryTensor{outputTensor},
		options,
	)
	if err != nil {
		inputTensor.Destroy()
		outputTensor.Destroy()
		return nil, fmt.Errorf("error creating ORT session: %w", err)
	}

	return &inference.Session{
		Session: session,
		Input:   inputTensor,
		Output:  outputTensor,
	}, nil
}

// Predict runs inference on the provided image
func (oe *ONNXDetector) Predict(ctx context.Context, img image.Image) (interface{}, error) {
	if oe.session == nil {
		return nil, fmt.Errorf("model not loaded")
	}

	// Check context cancellation
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	// Get the target input shape from config for preprocessing
	inputShape, ok := oe.inputShape.X, oe.inputShape.Y
	var targetImg image.Image = img

	// If a specific input shape is configured, resize to that first (this simulates preprocessing overhead)
	if ok && len(inputShape) == 2 && (inputShape[0] != img.Bounds().Dx() || inputShape[1] != img.Bounds().Dy()) {
		// This step simulates the preprocessing cost of resizing to the target resolution
		// In a real scenario, this would be the desired inference resolution
		// For benchmarking, we measure this cost separately from the ONNX model input preparation
		targetImg = img // In practice, you'd resize here, but we'll keep it simple for benchmarking
	}

	// Prepare input for ONNX model (always 640x640 for the model)
	err := inference.PrepareInput(targetImg, oe.session.Input)
	if err != nil {
		return nil, fmt.Errorf("failed to prepare input: %w", err)
	}

	// Run inference
	err = oe.session.Session.Run()
	if err != nil {
		return nil, fmt.Errorf("failed to run inference: %w", err)
	}

	// Process output - use original image dimensions for scaling
	detections := inference.ProcessInferenceOutput(
		oe.session.Output.GetData(),
		img.Bounds().Dx(),
		img.Bounds().Dy(),
	)

	return detections, nil
}

// Detect runs inference on the input image.
//
// Arguments:
//   - img: The image to detect objects in.
//
// Returns:
//   - []ObjectDetectionResult: The detected objects.
//   - error: An error if the detection fails.
func (oe *ONNXDetector) Detect(img gocv.Mat) ([]ObjectDetectionResult, error) {
	oe.mu.RLock()
	defer oe.mu.RUnlock()

	if !oe.initialized {
		return nil, fmt.Errorf("detector not initialized")
	}

	// Preprocess the image
	blob := oe.preprocessImage(img)
	defer blob.Close()

	// Run inference
	outputs := oe.net.Forward("")
	defer outputs.Close()

	// Postprocess the outputs
	size := img.Size()
	detections := oe.postprocessOutputs(outputs, image.Point{X: size[1], Y: size[0]})

	return detections, nil
}

// DetectROI runs inference on a specific region of interest
func (oe *ONNXDetector) DetectROI(img gocv.Mat, roi image.Rectangle) ([]ObjectDetectionResult, error) {
	// Extract the ROI from the image
	roiMat := img.Region(roi)
	defer roiMat.Close()

	// Run detection on the ROI
	detections, err := oe.Detect(roiMat)
	if err != nil {
		return nil, err
	}

	// Adjust bounding box coordinates to original image space
	for i := range detections {
		detections[i].Box = detections[i].Box.Add(roi.Min)
	}

	return detections, nil
}

// preprocessImage prepares the input image for the model
func (oe *ONNXDetector) preprocessImage(img gocv.Mat) gocv.Mat {
	// Resize image to model input size
	resized := gocv.NewMat()
	gocv.Resize(img, &resized, oe.inputShape, 0, 0, gocv.InterpolationLinear)
	defer resized.Close()

	// Convert to blob (normalize and change format)
	blob := gocv.BlobFromImage(resized, 1.0/255.0, oe.inputShape, gocv.NewScalar(0, 0, 0, 0), true, false)

	return blob
}

// postprocessOutputs processes the model outputs to extract detections
func (oe *ONNXDetector) postprocessOutputs(outputs gocv.Mat, originalSize image.Point) []ObjectDetectionResult {
	var detections []ObjectDetectionResult

	// Get output dimensions
	rows := outputs.Rows()
	cols := outputs.Cols()

	// Process each detection
	for i := 0; i < rows; i++ {
		// Get confidence scores for all classes
		confidence := outputs.GetFloatAt(i, 4)
		if confidence < oe.confidenceThreshold {
			continue
		}

		// // BuildResults converts raw outputs into typed results.
		// func (m *models.ClassManager) BuildResults(
		// 	style models.OutputClassGeneration,
		// 	classIdxs []int,
		// 	scores []float32,
		// 	bboxes [][4]float32,
		// ) ([]InferenceResult, error) {
		// 	n := len(classIdxs)
		// 	if len(scores) != n || len(bboxes) != n {
		// 		return nil, fmt.Errorf("mismatched lengths: idxs=%d scores=%d bboxes=%d", n, len(scores), len(bboxes))
		// 	}

		// 	results := make([]InferenceResult, n)
		// 	for i, idx := range classIdxs {
		// 		name, err := m.GetName(style, idx)
		// 		if err != nil {
		// 			return nil, err
		// 		}
		// 		results[i] = InferenceResult{
		// 			ClassIdx: idx,
		// 			Score:    scores[i],
		// 			BBox:     bboxes[i],
		// 			Label:    name,
		// 		}
		// 	}
		// 	return results, nil
		// }

		// Find the class with highest confidence
		classID := 0
		maxScore := float32(0)
		for j := 5; j < cols; j++ {
			score := outputs.GetFloatAt(i, j)
			if score > maxScore {
				maxScore = score
				classID = j - 5
			}
		}

		// Calculate final confidence
		finalConfidence := confidence * maxScore
		if finalConfidence < oe.confidenceThreshold {
			continue
		}

		// Get bounding box coordinates
		centerX := outputs.GetFloatAt(i, 0)
		centerY := outputs.GetFloatAt(i, 1)
		width := outputs.GetFloatAt(i, 2)
		height := outputs.GetFloatAt(i, 3)

		// Convert normalized coordinates to pixel coordinates
		x1 := int((centerX - width/2) * float32(originalSize.X))
		y1 := int((centerY - height/2) * float32(originalSize.Y))
		x2 := int((centerX + width/2) * float32(originalSize.X))
		y2 := int((centerY + height/2) * float32(originalSize.Y))

		// Ensure coordinates are within image bounds
		x1 = max(0, x1)
		y1 = max(0, y1)
		x2 = min(originalSize.X, x2)
		y2 = min(originalSize.Y, y2)

		// Create detection
		detection := ObjectDetectionResult{
			Box:       image.Rect(x1, y1, x2, y2),
			Score:     finalConfidence,
			ClassID:   classID,
			ClassName: oe.getClassByID(classID),
		}

		detections = append(detections, detection)
	}

	// Apply Non-Maximum Suppression
	detections = oe.applyNMS(detections)

	return detections
}

// applyNMS applies Non-Maximum Suppression to remove overlapping detections
func (oe *ONNXDetector) applyNMS(detections []ObjectDetectionResult) []ObjectDetectionResult {
	if len(detections) == 0 {
		return detections
	}

	// Sort by confidence score (descending)
	sort.Slice(detections, func(i, j int) bool {
		return detections[i].Score > detections[j].Score
	})

	var result []ObjectDetectionResult
	used := make([]bool, len(detections))

	for i := 0; i < len(detections); i++ {
		if used[i] {
			continue
		}

		result = append(result, detections[i])
		used[i] = true

		// Check overlap with remaining detections
		for j := i + 1; j < len(detections); j++ {
			if used[j] {
				continue
			}

			// Calculate IoU
			iou := calculateIoU(detections[i].Box, detections[j].Box)
			if iou > oe.nmsThreshold {
				used[j] = true
			}
		}
	}

	return result
}

// getClassByID returns the class name for a given class ID
func (oe *ONNXDetector) getClassByID(classID int) string {
	if classID >= 0 && classID < len(COCOClasses) {
		return COCOClasses[classID]
	}
	return fmt.Sprintf("unknown_%d", classID)
}

// Close releases resources
func (oe *ONNXDetector) Close() {
	oe.mu.Lock()
	defer oe.mu.Unlock()

	if !oe.net.Empty() {
		oe.net.Close()
	}
	oe.initialized = false
	log.Printf("ðŸ”’ ONNX detector closed")
}

// GetModelInfo returns information about the loaded model
func (oe *ONNXDetector) GetModelInfo() map[string]interface{} {
	return map[string]interface{}{
		"model_path":           oe.modelPath,
		"input_shape":          oe.inputShape,
		"confidence_threshold": oe.confidenceThreshold,
		"nms_threshold":        oe.nmsThreshold,
		"relevant_classes":     oe.GetRelevantClasses(),
		"initialized":          oe.initialized,
		"output_layers":        oe.outputNames,
		"classes":              oe.relevantClasses,
	}
}

// WarmUp runs inference on the model to warm up the cache.
//
// Arguments:
//   - runs: The number of times to run inference.
//
// Returns:
//   - error: An error if the warmup fails.
func (oe *ONNXDetector) WarmUp(runs int) error {
	for i := 0; i < runs; i++ {
		_, err := oe.Detect(gocv.NewMat())
		if err != nil {
			return err
		}
	}
	return nil
}
